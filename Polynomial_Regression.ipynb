{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True\n",
    "\n",
    "lr = 3e-4\n",
    "weight_decay = 0.1\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# 1D regression\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Apply degree 3 polynomial basis\n",
    "w = tf.get_variable(\"degree_three\", shape=[3, 1])\n",
    "b_x = tf.stack([tf.square(x), x, tf.ones_like(x)], axis=1)\n",
    "\n",
    "# debug message\n",
    "if debug:\n",
    "    w = tf.Print(w, [w, tf.shape(w)], \"w: \")\n",
    "    b_x = tf.Print(b_x, [b_x, tf.shape(b_x)], \"b(x): \")\n",
    "\n",
    "prediction = tf.squeeze(tf.matmul(b_x, w), 1)\n",
    "\n",
    "if debug:\n",
    "    prediction = tf.Print(prediction, [prediction, tf.shape(prediction)], \"prediction: \")\n",
    "\n",
    "train_loss = tf.nn.l2_loss(prediction - y) + weight_decay * tf.nn.l2_loss(w)\n",
    "opt = tf.train.AdamOptimizer(lr).minimize(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    x_val = np.random.uniform(-10.0, 10.0, size=100)\n",
    "    y_val = 5 * np.square(x_val) + 3\n",
    "    return x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3698537.25\n",
      "Training Loss: 3849734.5\n",
      "Training Loss: 3922191.5\n",
      "Training Loss: 3554153.5\n",
      "Training Loss: 3755294.0\n",
      "Training Loss: 4366348.0\n",
      "Training Loss: 3374259.0\n",
      "Training Loss: 3664145.5\n",
      "Training Loss: 3281304.5\n",
      "Training Loss: 3552452.0\n",
      "Training Loss: 3328184.75\n",
      "Training Loss: 2741936.0\n",
      "Training Loss: 3897767.5\n",
      "Training Loss: 3268132.0\n",
      "Training Loss: 3638548.25\n",
      "Training Loss: 3576723.0\n",
      "Training Loss: 3458470.5\n",
      "Training Loss: 3936925.5\n",
      "Training Loss: 4595909.0\n",
      "Training Loss: 4106305.25\n",
      "Training Loss: 3262260.0\n",
      "Training Loss: 3028592.5\n",
      "Training Loss: 3538419.0\n",
      "Training Loss: 3426201.25\n",
      "Training Loss: 3835703.0\n",
      "Training Loss: 4259432.0\n",
      "Training Loss: 2832068.75\n",
      "Training Loss: 4776229.0\n",
      "Training Loss: 4135988.75\n",
      "Training Loss: 3653889.0\n",
      "Training Loss: 3574996.5\n",
      "Training Loss: 3411087.5\n",
      "Training Loss: 3350537.0\n",
      "Training Loss: 4184582.0\n",
      "Training Loss: 3195400.5\n",
      "Training Loss: 4731355.5\n",
      "Training Loss: 3961145.5\n",
      "Training Loss: 3916389.0\n",
      "Training Loss: 3239177.5\n",
      "Training Loss: 4213102.0\n",
      "Training Loss: 3428119.0\n",
      "Training Loss: 3424268.5\n",
      "Training Loss: 3934938.75\n",
      "Training Loss: 4684156.0\n",
      "Training Loss: 3902919.0\n",
      "Training Loss: 3375952.0\n",
      "Training Loss: 3096176.5\n",
      "Training Loss: 3243627.5\n",
      "Training Loss: 4498409.0\n",
      "Training Loss: 3923464.5\n",
      "Training Loss: 4301768.5\n",
      "Training Loss: 3792799.75\n",
      "Training Loss: 3470109.25\n",
      "Training Loss: 4171703.5\n",
      "Training Loss: 2975393.0\n",
      "Training Loss: 3776367.0\n",
      "Training Loss: 3546906.5\n",
      "Training Loss: 4883758.0\n",
      "Training Loss: 4037423.0\n",
      "Training Loss: 3165281.25\n",
      "Training Loss: 3728310.5\n",
      "Training Loss: 3551355.0\n",
      "Training Loss: 3715243.5\n",
      "Training Loss: 3646830.5\n",
      "Training Loss: 3327987.25\n",
      "Training Loss: 4150811.0\n",
      "Training Loss: 4087288.5\n",
      "Training Loss: 3138589.25\n",
      "Training Loss: 2231680.0\n",
      "Training Loss: 4000510.25\n",
      "Training Loss: 4150041.0\n",
      "Training Loss: 3651949.25\n",
      "Training Loss: 3378522.0\n",
      "Training Loss: 4390431.5\n",
      "Training Loss: 3365143.5\n",
      "Training Loss: 3669919.0\n",
      "Training Loss: 3480313.25\n",
      "Training Loss: 3628508.0\n",
      "Training Loss: 2968199.25\n",
      "Training Loss: 3458818.25\n",
      "Training Loss: 3832104.0\n",
      "Training Loss: 3748409.75\n",
      "Training Loss: 3919906.0\n",
      "Training Loss: 3875470.0\n",
      "Training Loss: 3533954.5\n",
      "Training Loss: 3678366.5\n",
      "Training Loss: 4166155.0\n",
      "Training Loss: 3685135.5\n",
      "Training Loss: 3573346.5\n",
      "Training Loss: 3998293.75\n",
      "Training Loss: 3078433.5\n",
      "Training Loss: 3561904.5\n",
      "Training Loss: 2992055.75\n",
      "Training Loss: 3160455.0\n",
      "Training Loss: 3825273.0\n",
      "Training Loss: 4232765.0\n",
      "Training Loss: 3425677.5\n",
      "Training Loss: 4090620.5\n",
      "Training Loss: 4544322.5\n",
      "Training Loss: 4153399.25\n",
      "Training Loss: 3512224.75\n",
      "Training Loss: 3602113.5\n",
      "Training Loss: 3191102.5\n",
      "Training Loss: 4516743.0\n",
      "Training Loss: 3719118.75\n",
      "Training Loss: 3794175.0\n",
      "Training Loss: 2638675.0\n",
      "Training Loss: 3899479.25\n",
      "Training Loss: 4014760.75\n",
      "Training Loss: 3790700.5\n",
      "Training Loss: 3834251.0\n",
      "Training Loss: 4780152.5\n",
      "Training Loss: 3942614.75\n",
      "Training Loss: 3776751.75\n",
      "Training Loss: 2522936.5\n",
      "Training Loss: 4617639.0\n",
      "Training Loss: 3651659.0\n",
      "Training Loss: 3411006.75\n",
      "Training Loss: 4205513.0\n",
      "Training Loss: 3953709.25\n",
      "Training Loss: 3418093.0\n",
      "Training Loss: 3423233.0\n",
      "Training Loss: 3912227.5\n",
      "Training Loss: 4076394.75\n",
      "Training Loss: 4364058.0\n",
      "Training Loss: 3898279.75\n",
      "Training Loss: 3166388.5\n",
      "Training Loss: 4551259.5\n",
      "Training Loss: 3831322.5\n",
      "Training Loss: 4113738.0\n",
      "Training Loss: 3257571.0\n",
      "Training Loss: 3013263.25\n",
      "Training Loss: 3228158.0\n",
      "Training Loss: 3745814.5\n",
      "Training Loss: 4604828.0\n",
      "Training Loss: 3706937.5\n",
      "Training Loss: 3442870.0\n",
      "Training Loss: 3463855.5\n",
      "Training Loss: 2959586.25\n",
      "Training Loss: 3817705.5\n",
      "Training Loss: 3547343.0\n",
      "Training Loss: 3750493.75\n",
      "Training Loss: 4339476.0\n",
      "Training Loss: 5004516.0\n",
      "Training Loss: 3748827.0\n",
      "Training Loss: 3705670.0\n",
      "Training Loss: 3024089.5\n",
      "Training Loss: 4204708.0\n",
      "Training Loss: 4267914.0\n",
      "Training Loss: 4257785.0\n",
      "Training Loss: 2698367.0\n",
      "Training Loss: 4213828.0\n",
      "Training Loss: 2941456.5\n",
      "Training Loss: 3585125.5\n",
      "Training Loss: 3797131.25\n",
      "Training Loss: 4278372.0\n",
      "Training Loss: 3980958.5\n",
      "Training Loss: 3335897.0\n",
      "Training Loss: 3384936.75\n",
      "Training Loss: 4259149.0\n",
      "Training Loss: 3582644.0\n",
      "Training Loss: 3517644.5\n",
      "Training Loss: 3426590.0\n",
      "Training Loss: 4530078.0\n",
      "Training Loss: 3629578.0\n",
      "Training Loss: 3032914.0\n",
      "Training Loss: 3184066.75\n",
      "Training Loss: 3516904.5\n",
      "Training Loss: 3891252.0\n",
      "Training Loss: 3972369.0\n",
      "Training Loss: 3381877.0\n",
      "Training Loss: 3396032.75\n",
      "Training Loss: 3803300.0\n",
      "Training Loss: 3915411.5\n",
      "Training Loss: 3803117.0\n",
      "Training Loss: 3549880.5\n",
      "Training Loss: 4846134.0\n",
      "Training Loss: 3110426.5\n",
      "Training Loss: 3808802.0\n",
      "Training Loss: 3766567.75\n",
      "Training Loss: 3387235.0\n",
      "Training Loss: 4455203.0\n",
      "Training Loss: 3448153.5\n",
      "Training Loss: 3642613.75\n",
      "Training Loss: 4071973.0\n",
      "Training Loss: 3074795.0\n",
      "Training Loss: 3627469.0\n",
      "Training Loss: 3771773.5\n",
      "Training Loss: 3869676.25\n",
      "Training Loss: 4375713.0\n",
      "Training Loss: 2581374.75\n",
      "Training Loss: 3814635.5\n",
      "Training Loss: 3668970.25\n",
      "Training Loss: 3808531.5\n",
      "Training Loss: 3238550.75\n",
      "Training Loss: 3296224.0\n",
      "Training Loss: 3371672.5\n",
      "Training Loss: 3092637.0\n",
      "Training Loss: 4137516.75\n",
      "Training Loss: 3149279.25\n",
      "Training Loss: 3442735.25\n",
      "Training Loss: 4740341.0\n",
      "Training Loss: 3990982.0\n",
      "Training Loss: 3163437.25\n",
      "Training Loss: 4093333.5\n",
      "Training Loss: 3874349.75\n",
      "Training Loss: 3205286.5\n",
      "Training Loss: 4437816.5\n",
      "Training Loss: 3717953.25\n",
      "Training Loss: 2951814.5\n",
      "Training Loss: 3690983.25\n",
      "Training Loss: 3485892.25\n",
      "Training Loss: 3573460.5\n",
      "Training Loss: 3821067.0\n",
      "Training Loss: 3916401.0\n",
      "Training Loss: 4099519.5\n",
      "Training Loss: 3333165.0\n",
      "Training Loss: 3970888.75\n",
      "Training Loss: 3147982.5\n",
      "Training Loss: 3573208.5\n",
      "Training Loss: 3018575.5\n",
      "Training Loss: 3691338.25\n",
      "Training Loss: 3935989.75\n",
      "Training Loss: 4452864.0\n",
      "Training Loss: 4291418.0\n",
      "Training Loss: 3869177.0\n",
      "Training Loss: 3559942.25\n",
      "Training Loss: 2723799.0\n",
      "Training Loss: 3871246.75\n",
      "Training Loss: 3139549.0\n",
      "Training Loss: 4014281.0\n",
      "Training Loss: 2966001.75\n",
      "Training Loss: 3271496.5\n",
      "Training Loss: 3269722.0\n",
      "Training Loss: 4156340.0\n",
      "Training Loss: 3593554.0\n",
      "Training Loss: 4860645.0\n",
      "Training Loss: 3970821.75\n",
      "Training Loss: 3469718.0\n",
      "Training Loss: 4468308.5\n",
      "Training Loss: 3243463.5\n",
      "Training Loss: 4392918.5\n",
      "Training Loss: 3258894.0\n",
      "Training Loss: 3739472.75\n",
      "Training Loss: 3389280.5\n",
      "Training Loss: 4039289.0\n",
      "Training Loss: 4324770.0\n",
      "Training Loss: 3710099.5\n",
      "Training Loss: 3418030.0\n",
      "Training Loss: 3435474.0\n",
      "Training Loss: 3348128.5\n",
      "Training Loss: 3235051.5\n",
      "Training Loss: 4123460.0\n",
      "Training Loss: 3814059.5\n",
      "Training Loss: 4021681.5\n",
      "Training Loss: 4214506.0\n",
      "Training Loss: 3489235.5\n",
      "Training Loss: 3086440.0\n",
      "Training Loss: 4545434.0\n",
      "Training Loss: 2844444.25\n",
      "Training Loss: 3895141.0\n",
      "Training Loss: 3293455.25\n",
      "Training Loss: 3685385.5\n",
      "Training Loss: 3376197.25\n",
      "Training Loss: 4150379.5\n",
      "Training Loss: 3716596.5\n",
      "Training Loss: 3027022.5\n",
      "Training Loss: 3685526.0\n",
      "Training Loss: 3236884.5\n",
      "Training Loss: 3528391.0\n",
      "Training Loss: 3518509.25\n",
      "Training Loss: 3584336.0\n",
      "Training Loss: 3291921.0\n",
      "Training Loss: 3739458.75\n",
      "Training Loss: 4327780.0\n",
      "Training Loss: 4651106.0\n",
      "Training Loss: 3606441.5\n",
      "Training Loss: 3829514.0\n",
      "Training Loss: 3463643.0\n",
      "Training Loss: 2593484.0\n",
      "Training Loss: 4597877.0\n",
      "Training Loss: 2895879.0\n",
      "Training Loss: 2988413.75\n",
      "Training Loss: 3355713.0\n",
      "Training Loss: 4193785.5\n",
      "Training Loss: 3417966.0\n",
      "Training Loss: 4241136.5\n",
      "Training Loss: 4614511.0\n",
      "Training Loss: 3446834.0\n",
      "Training Loss: 3376481.5\n",
      "Training Loss: 4189806.75\n",
      "Training Loss: 3446291.0\n",
      "Training Loss: 3235057.0\n",
      "Training Loss: 3507286.0\n",
      "Training Loss: 3594396.5\n",
      "Training Loss: 3452061.0\n",
      "Training Loss: 2974335.0\n",
      "Training Loss: 4504459.0\n",
      "Training Loss: 3984468.5\n",
      "Training Loss: 4102202.75\n",
      "Training Loss: 4266508.5\n",
      "Training Loss: 3586692.25\n",
      "Training Loss: 3576045.0\n",
      "Training Loss: 3492377.0\n",
      "Training Loss: 3645027.75\n",
      "Training Loss: 3846469.5\n",
      "Training Loss: 4204910.0\n",
      "Training Loss: 3649292.5\n",
      "Training Loss: 3704168.5\n",
      "Training Loss: 3570816.75\n",
      "Training Loss: 3312326.5\n",
      "Training Loss: 3794119.75\n",
      "Training Loss: 3768314.5\n",
      "Training Loss: 3766997.0\n",
      "Training Loss: 3096578.5\n",
      "Training Loss: 3143128.25\n",
      "Training Loss: 3901833.5\n",
      "Training Loss: 2018025.75\n",
      "Training Loss: 3511797.25\n",
      "Training Loss: 4666885.5\n",
      "Training Loss: 3532711.75\n",
      "Training Loss: 3717125.0\n",
      "Training Loss: 3538102.0\n",
      "Training Loss: 3004602.25\n",
      "Training Loss: 3619227.5\n",
      "Training Loss: 2864775.5\n",
      "Training Loss: 3218251.5\n",
      "Training Loss: 3514000.5\n",
      "Training Loss: 3755390.5\n",
      "Training Loss: 3384302.25\n",
      "Training Loss: 4029132.5\n",
      "Training Loss: 3115874.0\n",
      "Training Loss: 4036023.5\n",
      "Training Loss: 3188362.0\n",
      "Training Loss: 3233854.75\n",
      "Training Loss: 3232961.25\n",
      "Training Loss: 3591536.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4277172.5\n",
      "Training Loss: 4471849.0\n",
      "Training Loss: 2936043.5\n",
      "Training Loss: 3610890.25\n",
      "Training Loss: 4148754.5\n",
      "Training Loss: 3975451.0\n",
      "Training Loss: 3325826.0\n",
      "Training Loss: 2079017.75\n",
      "Training Loss: 3802353.0\n",
      "Training Loss: 3322965.5\n",
      "Training Loss: 4601104.0\n",
      "Training Loss: 3848369.0\n",
      "Training Loss: 3517359.5\n",
      "Training Loss: 3285409.25\n",
      "Training Loss: 3458666.5\n",
      "Training Loss: 3067289.0\n",
      "Training Loss: 4505842.5\n",
      "Training Loss: 3943564.5\n",
      "Training Loss: 3291684.5\n",
      "Training Loss: 3615154.0\n",
      "Training Loss: 3524497.25\n",
      "Training Loss: 3704460.0\n",
      "Training Loss: 3332127.75\n",
      "Training Loss: 3498186.0\n",
      "Training Loss: 2917180.5\n",
      "Training Loss: 4378207.0\n",
      "Training Loss: 2902838.75\n",
      "Training Loss: 3397876.0\n",
      "Training Loss: 3767503.5\n",
      "Training Loss: 2959208.5\n",
      "Training Loss: 3459014.5\n",
      "Training Loss: 3342969.5\n",
      "Training Loss: 4159118.5\n",
      "Training Loss: 3769872.5\n",
      "Training Loss: 4206795.5\n",
      "Training Loss: 3850685.25\n",
      "Training Loss: 3743283.0\n",
      "Training Loss: 4467106.0\n",
      "Training Loss: 3176440.5\n",
      "Training Loss: 3344727.5\n",
      "Training Loss: 3398538.5\n",
      "Training Loss: 3456246.5\n",
      "Training Loss: 3007110.75\n",
      "Training Loss: 3259088.5\n",
      "Training Loss: 3664365.0\n",
      "Training Loss: 4497293.0\n",
      "Training Loss: 3482847.75\n",
      "Training Loss: 4443703.0\n",
      "Training Loss: 3981263.0\n",
      "Training Loss: 3428466.75\n",
      "Training Loss: 3411605.25\n",
      "Training Loss: 3686824.5\n",
      "Training Loss: 3642823.5\n",
      "Training Loss: 3728220.0\n",
      "Training Loss: 3495255.0\n",
      "Training Loss: 3752102.0\n",
      "Training Loss: 3279525.5\n",
      "Training Loss: 4348354.0\n",
      "Training Loss: 4040412.5\n",
      "Training Loss: 3805473.75\n",
      "Training Loss: 2831093.75\n",
      "Training Loss: 2985882.5\n",
      "Training Loss: 3016184.25\n",
      "Training Loss: 3828014.0\n",
      "Training Loss: 3710518.5\n",
      "Training Loss: 4373954.0\n",
      "Training Loss: 3442722.25\n",
      "Training Loss: 2871377.0\n",
      "Training Loss: 3852742.25\n",
      "Training Loss: 4140901.0\n",
      "Training Loss: 3530428.0\n",
      "Training Loss: 3971510.5\n",
      "Training Loss: 2939733.5\n",
      "Training Loss: 4203002.5\n",
      "Training Loss: 3066372.75\n",
      "Training Loss: 2861617.75\n",
      "Training Loss: 4433656.0\n",
      "Training Loss: 3715128.0\n",
      "Training Loss: 3672916.5\n",
      "Training Loss: 3954056.25\n",
      "Training Loss: 3228625.0\n",
      "Training Loss: 3066778.0\n",
      "Training Loss: 2567347.5\n",
      "Training Loss: 5095137.0\n",
      "Training Loss: 3071046.5\n",
      "Training Loss: 3864185.5\n",
      "Training Loss: 3620467.75\n",
      "Training Loss: 3360700.0\n",
      "Training Loss: 3634613.0\n",
      "Training Loss: 4056648.5\n",
      "Training Loss: 3938698.0\n",
      "Training Loss: 3287453.5\n",
      "Training Loss: 3735476.75\n",
      "Training Loss: 3991673.5\n",
      "Training Loss: 3987875.75\n",
      "Training Loss: 4442756.0\n",
      "Training Loss: 3955469.25\n",
      "Training Loss: 3410404.0\n",
      "Training Loss: 3761816.0\n",
      "Training Loss: 3325081.5\n",
      "Training Loss: 4044989.0\n",
      "Training Loss: 4459510.5\n",
      "Training Loss: 3730739.0\n",
      "Training Loss: 3666418.5\n",
      "Training Loss: 3721964.5\n",
      "Training Loss: 4909410.0\n",
      "Training Loss: 3098480.25\n",
      "Training Loss: 4088504.75\n",
      "Training Loss: 3871704.0\n",
      "Training Loss: 3728009.75\n",
      "Training Loss: 3661695.75\n",
      "Training Loss: 2969413.75\n",
      "Training Loss: 3519089.5\n",
      "Training Loss: 3743348.75\n",
      "Training Loss: 3838918.25\n",
      "Training Loss: 3955681.5\n",
      "Training Loss: 3554398.0\n",
      "Training Loss: 3352242.0\n",
      "Training Loss: 3753972.0\n",
      "Training Loss: 3298398.75\n",
      "Training Loss: 4575802.5\n",
      "Training Loss: 3764953.5\n",
      "Training Loss: 3039073.0\n",
      "Training Loss: 3933381.5\n",
      "Training Loss: 3730818.0\n",
      "Training Loss: 3029488.0\n",
      "Training Loss: 4006772.75\n",
      "Training Loss: 3928625.0\n",
      "Training Loss: 3513250.25\n",
      "Training Loss: 3558971.0\n",
      "Training Loss: 4268438.0\n",
      "Training Loss: 2604884.25\n",
      "Training Loss: 4226469.5\n",
      "Training Loss: 2944077.75\n",
      "Training Loss: 3998375.0\n",
      "Training Loss: 4654724.5\n",
      "Training Loss: 3857564.5\n",
      "Training Loss: 3503544.5\n",
      "Training Loss: 2669097.75\n",
      "Training Loss: 3496692.75\n",
      "Training Loss: 3439484.0\n",
      "Training Loss: 3630342.25\n",
      "Training Loss: 2958807.0\n",
      "Training Loss: 2645269.75\n",
      "Training Loss: 3628685.0\n",
      "Training Loss: 3678105.25\n",
      "Training Loss: 3252827.0\n",
      "Training Loss: 3073545.5\n",
      "Training Loss: 2778235.25\n",
      "Training Loss: 3777174.0\n",
      "Training Loss: 3348841.25\n",
      "Training Loss: 4335156.0\n",
      "Training Loss: 2772897.5\n",
      "Training Loss: 3150456.5\n",
      "Training Loss: 3286163.0\n",
      "Training Loss: 2990249.0\n",
      "Training Loss: 3188740.75\n",
      "Training Loss: 4096456.75\n",
      "Training Loss: 3086541.0\n",
      "Training Loss: 3713971.0\n",
      "Training Loss: 3438680.25\n",
      "Training Loss: 3341424.5\n",
      "Training Loss: 2886251.75\n",
      "Training Loss: 3492943.75\n",
      "Training Loss: 3836935.5\n",
      "Training Loss: 3371405.0\n",
      "Training Loss: 3309117.75\n",
      "Training Loss: 3506852.25\n",
      "Training Loss: 3363927.5\n",
      "Training Loss: 3444834.0\n",
      "Training Loss: 3269641.25\n",
      "Training Loss: 3324435.5\n",
      "Training Loss: 3851313.75\n",
      "Training Loss: 4618607.0\n",
      "Training Loss: 4043572.25\n",
      "Training Loss: 4418880.0\n",
      "Training Loss: 4268330.5\n",
      "Training Loss: 4031193.0\n",
      "Training Loss: 3908121.0\n",
      "Training Loss: 4172279.0\n",
      "Training Loss: 3547516.5\n",
      "Training Loss: 2890146.25\n",
      "Training Loss: 3481677.0\n",
      "Training Loss: 3042519.25\n",
      "Training Loss: 3131663.0\n",
      "Training Loss: 3295336.5\n",
      "Training Loss: 3699955.5\n",
      "Training Loss: 3558721.25\n",
      "Training Loss: 3459250.0\n",
      "Training Loss: 3693080.0\n",
      "Training Loss: 3591623.5\n",
      "Training Loss: 3177918.75\n",
      "Training Loss: 3589681.0\n",
      "Training Loss: 4501258.5\n",
      "Training Loss: 3986538.75\n",
      "Training Loss: 4064818.0\n",
      "Training Loss: 4578982.0\n",
      "Training Loss: 3354967.5\n",
      "Training Loss: 3388470.5\n",
      "Training Loss: 2612998.5\n",
      "Training Loss: 3497768.0\n",
      "Training Loss: 3332269.0\n",
      "Training Loss: 3755876.5\n",
      "Training Loss: 3352961.25\n",
      "Training Loss: 3175619.5\n",
      "Training Loss: 3647672.5\n",
      "Training Loss: 4250865.0\n",
      "Training Loss: 4565874.5\n",
      "Training Loss: 3428580.25\n",
      "Training Loss: 4346250.5\n",
      "Training Loss: 3623169.5\n",
      "Training Loss: 3394896.0\n",
      "Training Loss: 3719838.0\n",
      "Training Loss: 3702579.75\n",
      "Training Loss: 2925806.0\n",
      "Training Loss: 3293201.0\n",
      "Training Loss: 4206351.0\n",
      "Training Loss: 3390078.25\n",
      "Training Loss: 3412951.0\n",
      "Training Loss: 2869813.0\n",
      "Training Loss: 4347609.0\n",
      "Training Loss: 2762038.0\n",
      "Training Loss: 3687476.0\n",
      "Training Loss: 3130973.0\n",
      "Training Loss: 3400168.75\n",
      "Training Loss: 3807927.5\n",
      "Training Loss: 3859545.75\n",
      "Training Loss: 3571286.0\n",
      "Training Loss: 3591034.25\n",
      "Training Loss: 3871286.25\n",
      "Training Loss: 4015257.25\n",
      "Training Loss: 2896738.5\n",
      "Training Loss: 2534527.75\n",
      "Training Loss: 3651722.0\n",
      "Training Loss: 2942780.75\n",
      "Training Loss: 3691531.25\n",
      "Training Loss: 3564544.0\n",
      "Training Loss: 3356126.0\n",
      "Training Loss: 4266839.0\n",
      "Training Loss: 3038175.0\n",
      "Training Loss: 3149656.75\n",
      "Training Loss: 3707015.5\n",
      "Training Loss: 3443687.25\n",
      "Training Loss: 3394223.0\n",
      "Training Loss: 3099074.75\n",
      "Training Loss: 3637027.5\n",
      "Training Loss: 3896150.5\n",
      "Training Loss: 3232832.75\n",
      "Training Loss: 3797733.5\n",
      "Training Loss: 4000125.25\n",
      "Training Loss: 3269372.5\n",
      "Training Loss: 3447620.75\n",
      "Training Loss: 4286040.0\n",
      "Training Loss: 3436247.5\n",
      "Training Loss: 3251931.25\n",
      "Training Loss: 3200196.0\n",
      "Training Loss: 3254700.0\n",
      "Training Loss: 3423395.5\n",
      "Training Loss: 3724350.0\n",
      "Training Loss: 3033813.0\n",
      "Training Loss: 3531397.0\n",
      "Training Loss: 3540931.5\n",
      "Training Loss: 3913850.0\n",
      "Training Loss: 2654512.0\n",
      "Training Loss: 3812189.0\n",
      "Training Loss: 3963871.0\n",
      "Training Loss: 3772994.5\n",
      "Training Loss: 3440839.5\n",
      "Training Loss: 3847564.25\n",
      "Training Loss: 3274921.0\n",
      "Training Loss: 3403130.75\n",
      "Training Loss: 3293990.0\n",
      "Training Loss: 3590611.0\n",
      "Training Loss: 3624733.5\n",
      "Training Loss: 2197301.75\n",
      "Training Loss: 3785585.0\n",
      "Training Loss: 4098897.5\n",
      "Training Loss: 4183695.5\n",
      "Training Loss: 3075183.75\n",
      "Training Loss: 3092127.5\n",
      "Training Loss: 3511901.5\n",
      "Training Loss: 3027088.5\n",
      "Training Loss: 3595560.25\n",
      "Training Loss: 3249515.5\n",
      "Training Loss: 3606434.0\n",
      "Training Loss: 3246299.0\n",
      "Training Loss: 3836908.0\n",
      "Training Loss: 3853879.5\n",
      "Training Loss: 3000074.5\n",
      "Training Loss: 3914594.75\n",
      "Training Loss: 3005448.0\n",
      "Training Loss: 3001181.25\n",
      "Training Loss: 2904637.5\n",
      "Training Loss: 3783864.0\n",
      "Training Loss: 3307864.0\n",
      "Training Loss: 4505403.5\n",
      "Training Loss: 3056531.75\n",
      "Training Loss: 4328722.0\n",
      "Training Loss: 3037375.5\n",
      "Training Loss: 3655155.5\n",
      "Training Loss: 3783260.75\n",
      "Training Loss: 3953210.25\n",
      "Training Loss: 3810171.25\n",
      "Training Loss: 3313215.5\n",
      "Training Loss: 3274538.75\n",
      "Training Loss: 3587325.75\n",
      "Training Loss: 3605059.5\n",
      "Training Loss: 3699816.0\n",
      "Training Loss: 3418468.0\n",
      "Training Loss: 3597565.5\n",
      "Training Loss: 2860228.75\n",
      "Training Loss: 3531787.5\n",
      "Training Loss: 3501589.0\n",
      "Training Loss: 4311249.0\n",
      "Training Loss: 3627000.5\n",
      "Training Loss: 3930618.5\n",
      "Training Loss: 3367892.0\n",
      "Training Loss: 3895121.0\n",
      "Training Loss: 3822467.0\n",
      "Training Loss: 3299694.0\n",
      "Training Loss: 4156826.0\n",
      "Training Loss: 2926892.5\n",
      "Training Loss: 3755571.5\n",
      "Training Loss: 3185379.25\n",
      "Training Loss: 3116284.5\n",
      "Training Loss: 3168993.5\n",
      "Training Loss: 4163217.0\n",
      "Training Loss: 3392581.5\n",
      "Training Loss: 3657844.75\n",
      "Training Loss: 3918224.0\n",
      "Training Loss: 3202318.0\n",
      "Training Loss: 3958629.5\n",
      "Training Loss: 3199343.25\n",
      "Training Loss: 2861591.0\n",
      "Training Loss: 4496788.0\n",
      "Training Loss: 3825110.0\n",
      "Training Loss: 4812323.5\n",
      "Training Loss: 2802535.0\n",
      "Training Loss: 3830649.0\n",
      "Training Loss: 2437894.5\n",
      "Training Loss: 3419463.5\n",
      "Training Loss: 4625439.0\n",
      "Training Loss: 3316662.5\n",
      "Training Loss: 3702804.0\n",
      "Training Loss: 2645311.0\n",
      "Training Loss: 3596229.5\n",
      "Training Loss: 2877261.0\n",
      "Training Loss: 3241943.0\n",
      "Training Loss: 3818635.5\n",
      "Training Loss: 2890768.5\n",
      "Training Loss: 2764623.0\n",
      "Training Loss: 4179129.5\n",
      "Training Loss: 3470778.75\n",
      "Training Loss: 3103621.0\n",
      "Training Loss: 3540901.5\n",
      "Training Loss: 3748552.75\n",
      "Training Loss: 3446504.0\n",
      "Training Loss: 3240020.25\n",
      "Training Loss: 3308979.25\n",
      "Training Loss: 3456533.5\n",
      "Training Loss: 3630747.25\n",
      "Training Loss: 3048550.0\n",
      "Training Loss: 3934453.0\n",
      "Training Loss: 4633024.5\n",
      "Training Loss: 3683426.5\n",
      "Training Loss: 3240449.5\n",
      "Training Loss: 2905312.25\n",
      "Training Loss: 3899928.0\n",
      "Training Loss: 3223429.5\n",
      "Training Loss: 3789486.5\n",
      "Training Loss: 2560400.0\n",
      "Training Loss: 2994006.25\n",
      "Training Loss: 3134985.0\n",
      "Training Loss: 3650608.0\n",
      "Training Loss: 3847555.5\n",
      "Training Loss: 3833163.0\n",
      "Training Loss: 2960938.0\n",
      "Training Loss: 3953810.25\n",
      "Training Loss: 4211663.0\n",
      "Training Loss: 3838876.0\n",
      "Training Loss: 4242608.0\n",
      "Training Loss: 3128669.0\n",
      "Training Loss: 3330634.25\n",
      "Training Loss: 3432338.0\n",
      "Training Loss: 4152797.0\n",
      "Training Loss: 3284199.0\n",
      "Training Loss: 3346030.0\n",
      "Training Loss: 2896343.5\n",
      "Training Loss: 3549499.0\n",
      "Training Loss: 2931281.25\n",
      "Training Loss: 4254329.0\n",
      "Training Loss: 2834492.0\n",
      "Training Loss: 3486310.75\n",
      "Training Loss: 4086005.75\n",
      "Training Loss: 2979696.5\n",
      "Training Loss: 3512310.5\n",
      "Training Loss: 3106414.75\n",
      "Training Loss: 4150709.0\n",
      "Training Loss: 3083433.25\n",
      "Training Loss: 3426108.5\n",
      "Training Loss: 3837446.25\n",
      "Training Loss: 3816241.25\n",
      "Training Loss: 3804376.75\n",
      "Training Loss: 3593259.0\n",
      "Training Loss: 3123999.5\n",
      "Training Loss: 3159446.5\n",
      "Training Loss: 3519847.5\n",
      "Training Loss: 3866165.25\n",
      "Training Loss: 4286814.0\n",
      "Training Loss: 3316548.5\n",
      "Training Loss: 3030287.5\n",
      "Training Loss: 3783088.5\n",
      "Training Loss: 3159694.0\n",
      "Training Loss: 3361644.25\n",
      "Training Loss: 2883296.75\n",
      "Training Loss: 3553260.75\n",
      "Training Loss: 2757300.0\n",
      "Training Loss: 3133049.5\n",
      "Training Loss: 3965600.5\n",
      "Training Loss: 3887017.25\n",
      "Training Loss: 4168797.0\n",
      "Training Loss: 3325789.0\n",
      "Training Loss: 3294714.75\n",
      "Training Loss: 3941848.25\n",
      "Training Loss: 2612703.0\n",
      "Training Loss: 3704423.0\n",
      "Training Loss: 2457733.0\n",
      "Training Loss: 3717851.5\n",
      "Training Loss: 3326422.75\n",
      "Training Loss: 4037221.5\n",
      "Training Loss: 3554108.0\n",
      "Training Loss: 4371188.0\n",
      "Training Loss: 3787482.75\n",
      "Training Loss: 2841205.0\n",
      "Training Loss: 2669034.0\n",
      "Training Loss: 4149524.5\n",
      "Training Loss: 4095066.0\n",
      "Training Loss: 2882873.0\n",
      "Training Loss: 3086130.0\n",
      "Training Loss: 3902134.25\n",
      "Training Loss: 3383171.0\n",
      "Training Loss: 2920656.0\n",
      "Training Loss: 4279324.5\n",
      "Training Loss: 3434118.25\n",
      "Training Loss: 3722746.5\n",
      "Training Loss: 3010420.0\n",
      "Training Loss: 3680063.25\n",
      "Training Loss: 3104719.0\n",
      "Training Loss: 4286317.0\n",
      "Training Loss: 4358446.0\n",
      "Training Loss: 2535195.75\n",
      "Training Loss: 3811498.25\n",
      "Training Loss: 3580630.25\n",
      "Training Loss: 3972391.0\n",
      "Training Loss: 3325388.0\n",
      "Training Loss: 3347685.5\n",
      "Training Loss: 3304892.5\n",
      "Training Loss: 4101016.25\n",
      "Training Loss: 2875192.75\n",
      "Training Loss: 3163429.5\n",
      "Training Loss: 3027299.0\n",
      "Training Loss: 3808563.0\n",
      "Training Loss: 2959380.5\n",
      "Training Loss: 3625738.25\n",
      "Training Loss: 3583443.0\n",
      "Training Loss: 4680232.0\n",
      "Training Loss: 4460500.0\n",
      "Training Loss: 3725026.5\n",
      "Training Loss: 4135064.0\n",
      "Training Loss: 3483483.0\n",
      "Training Loss: 2490262.0\n",
      "Training Loss: 3573709.25\n",
      "Training Loss: 3859756.5\n",
      "Training Loss: 2836763.25\n",
      "Training Loss: 3529429.5\n",
      "Training Loss: 3239800.5\n",
      "Training Loss: 3821174.25\n",
      "Training Loss: 3608925.5\n",
      "Training Loss: 3960517.75\n",
      "Training Loss: 3270087.25\n",
      "Training Loss: 3286475.0\n",
      "Training Loss: 3283848.75\n",
      "Training Loss: 4056160.5\n",
      "Training Loss: 3524930.75\n",
      "Training Loss: 2777283.0\n",
      "Training Loss: 4367890.0\n",
      "Training Loss: 3133078.5\n",
      "Training Loss: 4658775.0\n",
      "Training Loss: 3417849.25\n",
      "Training Loss: 3368356.0\n",
      "Training Loss: 3117876.0\n",
      "Training Loss: 3091801.5\n",
      "Training Loss: 2758734.0\n",
      "Training Loss: 3442170.0\n",
      "Training Loss: 4594641.0\n",
      "Training Loss: 3325412.5\n",
      "Training Loss: 3184259.0\n",
      "Training Loss: 4096007.25\n",
      "Training Loss: 3943013.5\n",
      "Training Loss: 3834972.75\n",
      "Training Loss: 3448364.5\n",
      "Training Loss: 3291830.5\n",
      "Training Loss: 3056090.75\n",
      "Training Loss: 3723313.25\n",
      "Training Loss: 3213215.0\n",
      "Training Loss: 2863437.75\n",
      "Training Loss: 4170766.0\n",
      "Training Loss: 4309484.0\n",
      "Training Loss: 4482846.0\n",
      "Training Loss: 3580033.5\n",
      "Training Loss: 3662066.25\n",
      "Training Loss: 3599309.0\n",
      "Training Loss: 3954166.5\n",
      "Training Loss: 3457020.5\n",
      "Training Loss: 3308288.0\n",
      "Training Loss: 3514153.5\n",
      "Training Loss: 4241961.0\n",
      "Training Loss: 3483019.75\n",
      "Training Loss: 3179325.0\n",
      "Training Loss: 3282696.5\n",
      "Training Loss: 3486634.25\n",
      "Training Loss: 3263290.75\n",
      "Training Loss: 3118269.0\n",
      "Training Loss: 2872100.0\n",
      "Training Loss: 3743549.5\n",
      "Training Loss: 3673923.75\n",
      "Training Loss: 3911544.25\n",
      "Training Loss: 3216498.0\n",
      "Training Loss: 3307192.0\n",
      "Training Loss: 4339659.0\n",
      "Training Loss: 3230106.5\n",
      "Training Loss: 3572802.25\n",
      "Training Loss: 3263616.0\n",
      "Training Loss: 3712170.5\n",
      "Training Loss: 3503171.75\n",
      "Training Loss: 2790326.5\n",
      "Training Loss: 2508844.0\n",
      "Training Loss: 3414600.5\n",
      "Training Loss: 3824799.0\n",
      "Training Loss: 2804309.0\n",
      "Training Loss: 3584361.5\n",
      "Training Loss: 3048270.5\n",
      "Training Loss: 3381822.25\n",
      "Training Loss: 3414764.5\n",
      "Training Loss: 3236161.0\n",
      "Training Loss: 3468012.25\n",
      "Training Loss: 3003032.0\n",
      "Training Loss: 3961746.75\n",
      "Training Loss: 3744711.0\n",
      "Training Loss: 3330232.25\n",
      "Training Loss: 3459808.25\n",
      "Training Loss: 3066822.0\n",
      "Training Loss: 3589900.0\n",
      "Training Loss: 3651885.5\n",
      "Training Loss: 3807574.75\n",
      "Training Loss: 3374449.5\n",
      "Training Loss: 3589450.5\n",
      "Training Loss: 3566334.75\n",
      "Training Loss: 4119747.5\n",
      "Training Loss: 3253910.25\n",
      "Training Loss: 3281690.0\n",
      "Training Loss: 3311803.0\n",
      "Training Loss: 4575745.5\n",
      "Training Loss: 4387195.0\n",
      "Training Loss: 3409685.5\n",
      "Training Loss: 4025216.5\n",
      "Training Loss: 3824042.5\n",
      "Training Loss: 3208326.5\n",
      "Training Loss: 3065152.25\n",
      "Training Loss: 4287903.0\n",
      "Training Loss: 3196797.25\n",
      "Training Loss: 4267569.0\n",
      "Training Loss: 3734038.5\n",
      "Training Loss: 3824156.5\n",
      "Training Loss: 4118025.5\n",
      "Training Loss: 3646202.0\n",
      "Training Loss: 3873753.75\n",
      "Training Loss: 3114407.5\n",
      "Training Loss: 4040462.5\n",
      "Training Loss: 2931604.5\n",
      "Training Loss: 3301814.5\n",
      "Training Loss: 3239789.5\n",
      "Training Loss: 4107539.0\n",
      "Training Loss: 4231703.0\n",
      "Training Loss: 4020569.0\n",
      "Training Loss: 3715072.0\n",
      "Training Loss: 3375126.0\n",
      "Training Loss: 3270516.25\n",
      "Training Loss: 3707570.0\n",
      "Training Loss: 3875528.5\n",
      "Training Loss: 4210256.0\n",
      "Training Loss: 4570263.0\n",
      "Training Loss: 3874924.0\n",
      "Training Loss: 3210549.0\n",
      "Training Loss: 3577250.5\n",
      "Training Loss: 3206810.5\n",
      "Training Loss: 3907781.75\n",
      "Training Loss: 4143437.25\n",
      "Training Loss: 3529162.5\n",
      "Training Loss: 2835243.5\n",
      "Training Loss: 3466633.0\n",
      "Training Loss: 3026108.75\n",
      "Training Loss: 3983139.25\n",
      "Training Loss: 4153663.75\n",
      "Training Loss: 3143388.75\n",
      "Training Loss: 2857243.25\n",
      "Training Loss: 3546104.0\n",
      "Training Loss: 3913084.5\n",
      "Training Loss: 3549246.75\n",
      "Training Loss: 3624897.5\n",
      "Training Loss: 2849925.75\n",
      "Training Loss: 3106333.75\n",
      "Training Loss: 2978013.0\n",
      "Training Loss: 3152770.0\n",
      "Training Loss: 3926717.75\n",
      "Training Loss: 2624791.25\n",
      "Training Loss: 3004751.75\n",
      "Training Loss: 3369159.25\n",
      "Training Loss: 2968826.5\n",
      "Training Loss: 3302585.25\n",
      "Training Loss: 3043394.0\n",
      "Training Loss: 2751939.0\n",
      "Training Loss: 4124793.0\n",
      "Training Loss: 2276006.5\n",
      "Training Loss: 2759636.5\n",
      "Training Loss: 3265601.5\n",
      "Training Loss: 3328234.0\n",
      "Training Loss: 3622919.5\n",
      "Training Loss: 3690514.5\n",
      "Training Loss: 3683864.0\n",
      "Training Loss: 2409243.0\n",
      "Training Loss: 3860546.0\n",
      "Training Loss: 2489327.0\n",
      "Training Loss: 3582434.5\n",
      "Training Loss: 4333281.0\n",
      "Training Loss: 2983517.5\n",
      "Training Loss: 3256875.0\n",
      "Training Loss: 3960901.5\n",
      "Training Loss: 4114858.0\n",
      "Training Loss: 4566536.0\n",
      "Training Loss: 3428819.5\n",
      "Training Loss: 3181276.0\n",
      "Training Loss: 3846288.25\n",
      "Training Loss: 2597957.5\n",
      "Training Loss: 3414506.0\n",
      "Training Loss: 3147933.0\n",
      "Training Loss: 3191358.25\n",
      "Training Loss: 2658966.0\n",
      "Training Loss: 3310575.0\n",
      "Training Loss: 3537287.25\n",
      "Training Loss: 3676787.5\n",
      "Training Loss: 2838567.25\n",
      "Training Loss: 3680659.0\n",
      "Training Loss: 2622530.75\n",
      "Training Loss: 3884244.25\n",
      "Training Loss: 4390745.5\n",
      "Training Loss: 3109483.5\n",
      "Training Loss: 3163869.0\n",
      "Training Loss: 2943787.5\n",
      "Training Loss: 3829863.25\n",
      "Training Loss: 3555495.5\n",
      "Training Loss: 2435061.5\n",
      "Training Loss: 3511680.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(epochs):\n",
    "        inputs, targets = generate_data()\n",
    "        _, loss_val = sess.run([opt, train_loss], {x: inputs, y: targets})\n",
    "        print(\"Training Loss: {}\".format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
